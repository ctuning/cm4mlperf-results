[
  {
    "Accuracy": "ROUGE1: 45.4369  ROUGE2: 23.2825  ROUGEL: 30.4229  TOKENS_PER_SAMPLE: 145.9  gsm8k_accuracy: 73.48  mbxp_accuracy: 59.94",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 50123.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "293f2bbcefb540ef",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.2751  ROUGE2: 23.1818  ROUGEL: 30.3401  TOKENS_PER_SAMPLE: 145.5  gsm8k_accuracy: 73.2  mbxp_accuracy: 60.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 7735.98,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "21f03d1bd4e54cbd",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.385  ROUGE2: 23.2684  ROUGEL: 30.2469  TOKENS_PER_SAMPLE: 146.2  gsm8k_accuracy: 72.94  mbxp_accuracy: 59.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 15570.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "fc77868c4912495c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4575  ROUGE2: 23.2383  ROUGEL: 30.3743  TOKENS_PER_SAMPLE: 145.8  gsm8k_accuracy: 73.26  mbxp_accuracy: 59.76",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Sustainable_Metal_Cloud",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 57054.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "8579152bbdf54283",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 22.048188322528272  exact_match: 90.08918266048973  TOKENS_PER_SAMPLE: 656.1",
    "Availability": "preview",
    "Division": "closed",
    "Location": "closed/Google/results/B200-SXM-180GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Google",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 847.484,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Debian 12",
    "uid": "3ece32fe137c48a6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/B200-SXM-180GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.4486  ROUGE2: 23.1917  ROUGEL: 30.2808  TOKENS_PER_SAMPLE: 146.4  gsm8k_accuracy: 73.12  mbxp_accuracy: 60.4",
    "Availability": "preview",
    "Division": "closed",
    "Location": "closed/Google/results/B200-SXM-180GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Google",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 122411,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Debian 12",
    "uid": "db1b544b901d4802",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/B200-SXM-180GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGEL: 21.677865835546807  exact_match: 90.07546988749174  TOKENS_PER_SAMPLE: 648.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 276.43,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "92bf5e8be5df48e9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.3268  ROUGE2: 23.1698  ROUGEL: 30.2987  TOKENS_PER_SAMPLE: 145.6  gsm8k_accuracy: 73.48  mbxp_accuracy: 59.78",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 59100.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "911eaa1468d14e98",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.3856  ROUGE2: 23.246  ROUGEL: 30.2962  TOKENS_PER_SAMPLE: 146.2  gsm8k_accuracy: 73.38  mbxp_accuracy: 59.82",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 7771.75,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "ee57c54b3ddc4f0f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.565117800148553  exact_match: 90.08828921244209  TOKENS_PER_SAMPLE: 624.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 261.98,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "0822f0e82abc46fe",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4321  ROUGE2: 23.1829  ROUGEL: 30.3742  TOKENS_PER_SAMPLE: 146.5  gsm8k_accuracy: 73.86  mbxp_accuracy: 59.88",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 50924.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "2c82b44509174412",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.3176  ROUGE2: 23.1416  ROUGEL: 30.2829  TOKENS_PER_SAMPLE: 145.4  gsm8k_accuracy: 73.52  mbxp_accuracy: 60.02",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 13864.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "45080711af2445af",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4444  ROUGE2: 23.2749  ROUGEL: 30.3007  TOKENS_PER_SAMPLE: 146.3  gsm8k_accuracy: 73.24  mbxp_accuracy: 59.88",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 8042.27,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "a03166e2c41c4058",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.975590204593704  exact_match: 90.07971211118463  TOKENS_PER_SAMPLE: 654.0",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 845.82,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "82638956869d494c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.4131  ROUGE2: 23.183  ROUGEL: 30.2308  TOKENS_PER_SAMPLE: 145.5  gsm8k_accuracy: 73.24  mbxp_accuracy: 59.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 126845,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "94db818542d74262",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGEL: 21.959911702352596  exact_match: 90.1118795499669  TOKENS_PER_SAMPLE: 656.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x72_TRT_Triton/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "NVIDIA",
    "Platform": "GB200-NVL72_GB200-186GB_aarch64x72_TRT_Triton",
    "Result": 8850.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": " (72x GB200-186GB_aarch64, TensorRT, Triton)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GB200",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GB200 NVL72",
    "number_of_nodes": 18,
    "operating_system": "Ubuntu 24.04",
    "uid": "ac70924babe74f05",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x72_TRT_Triton/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGEL: 21.6585777555619  exact_match: 90.07448378557245  TOKENS_PER_SAMPLE: 647.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 291.374,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b559dbc4049a4257",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4092  ROUGE2: 23.1612  ROUGEL: 30.3199  TOKENS_PER_SAMPLE: 144.4  gsm8k_accuracy: 73.4  mbxp_accuracy: 59.66",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 61802,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "38a40a8c0dc64e65",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.3758  ROUGE2: 23.2396  ROUGEL: 30.3046  TOKENS_PER_SAMPLE: 146.3  gsm8k_accuracy: 73.72  mbxp_accuracy: 60.0",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Oracle/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Oracle",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 59149.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LT",
    "uid": "365aa7a40bbf4c87",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Oracle/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.678324128995275  exact_match: 90.0528821972204  TOKENS_PER_SAMPLE: 650.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 291.183,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "7a5b4e1f795a4b5a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.63593824193972  exact_match: 89.99383520847121  TOKENS_PER_SAMPLE: 644.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 288.601,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "e11ef180da714c69",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4078  ROUGE2: 23.2531  ROUGEL: 30.3504  TOKENS_PER_SAMPLE: 146.3  gsm8k_accuracy: 74.04  mbxp_accuracy: 59.78",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 60205,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "a69049e2022842e4",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.989455826099128  exact_match: 90.08294837855725  TOKENS_PER_SAMPLE: 654.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/coreweave/results/GB200-NVL_GB200-NVL-186GB_aarch64x4_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "coreweave",
    "Platform": "GB200-NVL_GB200-NVL-186GB_aarch64x4_TRT",
    "Result": 522.12,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": " (4 x GB200-186GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GB200",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GB200 NVL72, 4-GPU node system",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "77860c0b14fe470c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/coreweave/results/GB200-NVL_GB200-NVL-186GB_aarch64x4_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.5484  ROUGE2: 23.3628  ROUGEL: 30.443  TOKENS_PER_SAMPLE: 146.5  gsm8k_accuracy: 72.96  mbxp_accuracy: 59.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_H100_SXMx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Cisco",
    "Platform": "C885A_H100_SXMx8_TRT",
    "Result": 53299.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "aa0bf0890a604070",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_H100_SXMx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.698924776508495  exact_match: 90.06501323626736  TOKENS_PER_SAMPLE: 657.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Cisco",
    "Platform": "C885A_M8_H200_SXM_141GBx8_TRT",
    "Result": 294.275,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4035ead051d24b6e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 22.071095668630047  exact_match: 90.1108107213766  TOKENS_PER_SAMPLE: 655.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Supermicro",
    "Platform": "SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT",
    "Result": 1080.31,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "e495d45bc67a48c5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.4923  ROUGE2: 23.2325  ROUGEL: 30.3051  TOKENS_PER_SAMPLE: 146.6  gsm8k_accuracy: 73.02  mbxp_accuracy: 59.72",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT",
    "Result": 128961,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "9a9b54ae6a9148dc",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.4991  ROUGE2: 23.3568  ROUGEL: 30.4444  TOKENS_PER_SAMPLE: 146.2  gsm8k_accuracy: 73.64  mbxp_accuracy: 60.14",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 59627.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "6e18779306a14ee8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.5543  ROUGE2: 23.3316  ROUGEL: 30.4478  TOKENS_PER_SAMPLE: 146.1  gsm8k_accuracy: 73.72  mbxp_accuracy: 60.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/ARS_111GL_NHR_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "ARS_111GL_NHR_TRT",
    "Result": 7182.77,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ARS-111GL-NHR (1x GH200-96GB_aarch64)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 96GB HBM3",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "dcad90e31af24231",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/ARS_111GL_NHR_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.654998586911255  exact_match: 90.01192256783585  TOKENS_PER_SAMPLE: 646.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 290.093,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0c17fc5c524c4f75",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.518  ROUGE2: 23.1971  ROUGEL: 30.3919  TOKENS_PER_SAMPLE: 145.8  gsm8k_accuracy: 73.0  mbxp_accuracy: 59.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 59647.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b660c7717af244d2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 22.044077554195237  exact_match: 90.11247518199869  TOKENS_PER_SAMPLE: 657.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT",
    "Result": 1057.52,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "79885b50e1c04658",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.4541  ROUGE2: 23.1973  ROUGEL: 30.2838  TOKENS_PER_SAMPLE: 146.3  gsm8k_accuracy: 72.94  mbxp_accuracy: 59.96",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT",
    "Result": 129047,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "0854ac0d1dda4563",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 45.416  ROUGE2: 23.2389  ROUGEL: 30.246  TOKENS_PER_SAMPLE: 146.8  gsm8k_accuracy: 73.1  mbxp_accuracy: 59.88",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Lenovo",
    "Platform": "SR675v3_H200_SXMx4_TRT",
    "Result": 29301.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR675 V3 (4x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9655 96-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "96545fa6b2b44e79",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4582  ROUGE2: 23.1907  ROUGEL: 30.36  TOKENS_PER_SAMPLE: 146.9  gsm8k_accuracy: 72.98  mbxp_accuracy: 59.74",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H100-NVL-94GBx4_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Lenovo",
    "Platform": "H100-NVL-94GBx4_TRT",
    "Result": 19334.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR650a V4 (4x H100-NVL-94GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "Intel(R) Xeon(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d98d70d330ec4bd3",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H100-NVL-94GBx4_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4078  ROUGE2: 23.1492  ROUGEL: 30.338  TOKENS_PER_SAMPLE: 144.7  gsm8k_accuracy: 73.02  mbxp_accuracy: 60.0",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 61074.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d4f9a07da34d44a6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.3802  ROUGE2: 23.1803  ROUGEL: 30.3072  TOKENS_PER_SAMPLE: 145.6  gsm8k_accuracy: 73.0  mbxp_accuracy: 60.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 61317.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "20b70540f58f4fbc",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.5359  ROUGE2: 23.2614  ROUGEL: 30.4398  TOKENS_PER_SAMPLE: 146.4  gsm8k_accuracy: 73.12  mbxp_accuracy: 59.92",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/vXE9680_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Dell",
    "Platform": "vXE9680_H100_SXM_80GBx8_TRT",
    "Result": 50896,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB,VMware ESXi 8.0.3) ",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "Virtualized NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel Xeon Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "VM Specifications 64vCPU out of 96 and memory of 1TB GB out of 2TB",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1, VMware ESXi 8.0.3 2441450",
    "uid": "0d6bbc1f9c994b4b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/vXE9680_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.67381013091784  exact_match: 89.96315023163469  TOKENS_PER_SAMPLE: 650.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 291.486,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "1fdecb2dccd44d05",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 45.4712  ROUGE2: 23.1931  ROUGEL: 30.3668  TOKENS_PER_SAMPLE: 146.4  gsm8k_accuracy: 73.2  mbxp_accuracy: 60.06",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 58972.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "67a5130581f24b07",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 21.578037465211967  exact_match: 90.02991065519522  TOKENS_PER_SAMPLE: 646.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama3_1-405b/Server",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 277.125,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "84d8a4daa9564bde",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama3_1-405b/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": 45.3594,
    "Accuracy_div_100": 0.45359,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Dell",
    "Platform": "XE8640_H100_SXM_80GBx4_TRT",
    "Result": 25707,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE8640 (4x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 9.3.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.6",
    "uid": "6b1a81891b534703",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Dell/results/XE8640_H100_SXM_80GBx4_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3591,
    "Accuracy_div_100": 0.45359,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Fujitsu",
    "Platform": "GX2560M7_H100_SXM_80GBx4_TRT",
    "Result": 25358.5,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "GX2560M7_H100_SXM_80GBx4 (4x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.4",
    "uid": "101d88bd044d4f5c",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Fujitsu/results/GX2560M7_H100_SXM_80GBx4_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3551,
    "Accuracy_div_100": 0.45355,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/GIGABYTE_G593_SD1_H200_SXM_141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "GigaComputing",
    "Platform": "GIGABYTE_G593_SD1_H200_SXM_141GBx8_TRT",
    "Result": 57174.1,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "GIGABYTE G593-SD1",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "4987b597e0de4d08",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/GigaComputing/results/GIGABYTE_G593_SD1_H200_SXM_141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3561,
    "Accuracy_div_100": 0.45356,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/DGX-H100_H100-SXM-80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Google",
    "Platform": "DGX-H100_H100-SXM-80GBx8_TRT",
    "Result": 50798.8,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8481C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "fbccb84be1a64f80",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Google/results/DGX-H100_H100-SXM-80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3563,
    "Accuracy_div_100": 0.45356,
    "Availability": "preview",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 7450.72,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.2",
    "uid": "6aec66daf3e54ef4",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3565,
    "Accuracy_div_100": 0.45356,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 50798.5,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "22.04.4 LTS",
    "uid": "21e6bb3cc0024989",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3537,
    "Accuracy_div_100": 0.45354,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "NVIDIA",
    "Platform": "DGX-H100_H100-SXM-80GBx8_TRT",
    "Result": 50795.6,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "59d188a9a30b4124",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3563,
    "Accuracy_div_100": 0.45356,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 7450.27,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA GH200 NVL2 Platform (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.2",
    "uid": "4c1b50c2f5de41b9",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3535,
    "Accuracy_div_100": 0.45353,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 57177.2,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "0eec91a7e59a4ce4",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.351,
    "Accuracy_div_100": 0.45351,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H100_SXM_80GBx8_TRT",
    "Result": 50099.5,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS",
    "uid": "c00686a346cb46f7",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.351,
    "Accuracy_div_100": 0.45351,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_8125GS_TNHR_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "AS_8125GS_TNHR_H100_SXM_80GBx8_TRT",
    "Result": 49864,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "AS-8125GS-TNHR (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9474F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS",
    "uid": "ba1cdbad91f44417",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Supermicro/results/AS_8125GS_TNHR_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3565,
    "Accuracy_div_100": 0.45356,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_TNHR2_LCC_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_TNHR2_LCC_H100_SXM_80GBx8_TRT",
    "Result": 51028.4,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-TNHR2-LCC (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS",
    "uid": "116994ce1b2a4d6e",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Supermicro/results/SYS_421GE_TNHR2_LCC_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3545,
    "Accuracy_div_100": 0.45355,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H100_SXM_80GBx8_TRT",
    "Result": 50676.9,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS",
    "uid": "ae74f644f26e4cda",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H100_SXM_80GBx8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 45.3528,
    "Accuracy_div_100": 0.45353,
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Sustainable_Metal_Cloud/results/SMC_H100_SXM_80GBX8_TRT/mixtral-8x7b/Server",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Sustainable_Metal_Cloud",
    "Platform": "SMC_H100_SXM_80GBX8_TRT",
    "Result": 50797.2,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "SMC H100 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "d618542fd89e47a8",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/closed/Sustainable_Metal_Cloud/results/SMC_H100_SXM_80GBX8_TRT/mixtral-8x7b/Server",
    "version": "v4.1"
  }
]
