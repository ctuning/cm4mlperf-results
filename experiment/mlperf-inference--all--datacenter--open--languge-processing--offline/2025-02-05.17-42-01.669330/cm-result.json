[
  {
    "Accuracy": "ROUGEL: 21.658590925735947  exact_match: 90.0034579748511  TOKENS_PER_SAMPLE: 648.3",
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama3_1-405b/Offline",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 428.895,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "2d1141484ee542f2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama3_1-405b/Offline",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGEL: 18.9480793556203  exact_match: 90.14579086697552  TOKENS_PER_SAMPLE: 641.1",
    "Availability": "available",
    "Division": "open",
    "Location": "open/Krai/results/xe9680_mi300x_x8_sglang/deepseek-v3/offline",
    "MlperfModel": "llama3.1-405b",
    "Model": "deepseek-v3",
    "Organization": "Krai",
    "Platform": "xe9680_mi300x_x8_sglang",
    "Result": 243.449,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "Dell PowerEdge XE9680 (8x MI300X-192GB, SGLang v0.4.3)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI300X 192GB HBM3",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "SGLang v0.4.3",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 40,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8460Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Powered by the KRAI X automation technology",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS (Jammy Jellyfish)",
    "uid": "2aa2a894c9b643e5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/Krai/results/xe9680_mi300x_x8_sglang/deepseek-v3/offline",
    "version": "v5.0",
    "weight_data_types": "UNSET"
  },
  {
    "Accuracy": "ROUGEL: 21.59129621595158  exact_match: 90.09433818663138  TOKENS_PER_SAMPLE: 641.0",
    "Availability": "available",
    "Division": "open",
    "Location": "open/Krai/results/xe9680_mi300x_x8_vllm_073/llama3_1-405b/offline",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Krai",
    "Platform": "xe9680_mi300x_x8_vllm_073",
    "Result": 188.604,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "Dell PowerEdge XE9680 (8x MI300X-192GB, vLLM v0.7.3)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI300X 192GB HBM3",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "vLLM v0.7.3",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 40,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8460Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Powered by the KRAI X automation technology",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS (Jammy Jellyfish)",
    "uid": "278fc30372ec412e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/Krai/results/xe9680_mi300x_x8_vllm_073/llama3_1-405b/offline",
    "version": "v5.0",
    "weight_data_types": "fp16"
  },
  {
    "Accuracy": "ROUGEL: 21.66444823059425  exact_match: 90.10726671078757  TOKENS_PER_SAMPLE: 632.1",
    "Availability": "available",
    "Division": "open",
    "Location": "open/Krai/results/xe9680_mi300x_x8_vllm_073/llama3_1-405b-fp8_dyn/offline",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b-fp8_dyn",
    "Organization": "Krai",
    "Platform": "xe9680_mi300x_x8_vllm_073",
    "Result": 278.649,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "Dell PowerEdge XE9680 (8x MI300X-192GB, vLLM v0.7.3)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI300X 192GB HBM3",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "vLLM v0.7.3",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 40,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8460Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Powered by the KRAI X automation technology",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS (Jammy Jellyfish)",
    "uid": "c7aac900041c4d1a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/Krai/results/xe9680_mi300x_x8_vllm_073/llama3_1-405b-fp8_dyn/offline",
    "version": "v5.0",
    "weight_data_types": "fp16"
  },
  {
    "Accuracy": "ROUGEL: 21.634266581485935  exact_match: 90.12628722700197  TOKENS_PER_SAMPLE: 656.3",
    "Availability": "available",
    "Division": "open",
    "Location": "open/Krai/results/xd670_h200_x8_sglang/llama3_1-405b-fp8_dyn/offline",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b-fp8_dyn",
    "Organization": "Krai",
    "Platform": "xd670_h200_x8_sglang",
    "Result": 326.537,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "HPE Cray XD670 (8x H200-SXM-141GB, SGLang v0.4.3)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "SGLang v0.4.3",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Powered by the KRAI X automation technology",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "4d99d145d7da4526",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/Krai/results/xd670_h200_x8_sglang/llama3_1-405b-fp8_dyn/offline",
    "version": "v5.0",
    "weight_data_types": "fp16"
  },
  {
    "Accuracy": "ROUGE1: 45.6315  ROUGE2: 23.4063  ROUGEL: 30.5125  TOKENS_PER_SAMPLE: 144.1  gsm8k_accuracy: 73.06  mbxp_accuracy: 60.38",
    "Availability": "available",
    "Division": "open",
    "Location": "open/Lambda/results/B200-SXM-180GBx8_TRT/mixtral-8x7b/Offline",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "Lambda",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 121886,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "19c4e141612f404c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/Lambda/results/B200-SXM-180GBx8_TRT/mixtral-8x7b/Offline",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGEL: 21.738071137617286  exact_match: 90.02873593646592  TOKENS_PER_SAMPLE: 660.4",
    "Availability": "available",
    "Division": "open",
    "Location": "open/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama3_1-405b/Offline",
    "MlperfModel": "llama3.1-405b",
    "Model": "llama3_1-405b",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 564.696,
    "Result_Units": "Tokens/s",
    "Scenario": "Offline",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "ac75b0378c7e4742",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/open/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama3_1-405b/Offline",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": 45.4026,
    "Accuracy_div_100": 0.45403,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx4_TRT/mixtral-8x7b/Offline",
    "MlperfModel": "mixtral-8x7b",
    "Model": "mixtral-8x7b",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_H100_NVL_94GBx4_TRT",
    "Result": 18839,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Gen11 (4x H100-NVL-94GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "INTEL(R) XEON(R) GOLD 6530",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS",
    "uid": "90112b5c9bee4820",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/open/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx4_TRT/mixtral-8x7b/Offline",
    "version": "v4.1"
  }
]
