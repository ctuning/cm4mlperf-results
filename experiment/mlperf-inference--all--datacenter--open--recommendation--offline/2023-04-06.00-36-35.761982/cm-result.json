[
  {
    "Accuracy": 62.297,
    "Accuracy_div_100": 0.62297,
    "Availability": "available",
    "Division": "open",
    "Location": "open/CTuning/results/GATE_Overflow_Intel_Sapphire_Rapids-nvidia_original-gpu-tensorrt-vdefault-default_config/dlrm-v2-99/offline",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "CTuning",
    "Platform": "GATE_Overflow_Intel_Sapphire_Rapids-nvidia_original-gpu-tensorrt-vdefault-default_config",
    "Result": 2988.18,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "GATEOverflow Intel Sapphire Rapids (2x RTX 4090)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerators_per_node": 2,
    "compliance": 1,
    "errors": 0,
    "framework": "Nvidia inference implementation with CM API, TensorRT v9.0.0",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "Intel(R) Xeon(R) w7-2495X",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v1.6.2. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04 (linux-6.2.0-39-generic-glibc2.31)",
    "uid": "8ec5f74a6da445d1",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/CTuning/results/GATE_Overflow_Intel_Sapphire_Rapids-nvidia_original-gpu-tensorrt-vdefault-default_config/dlrm-v2-99/offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 80.23400808344086,
    "Accuracy_div_100": 0.80234,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Dell/results/1-node-2S-SPR-PyTorch-INT8/dlrm-v2-99/Offline",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Dell",
    "Platform": "1-node-2S-SPR-PyTorch-INT8",
    "Result": 5016.63,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "Dell PowerEdge Server R760 (Intel Xeon Platinum 8480+)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 1,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v3.1",
    "has_power": false,
    "host_processor_core_count": 224,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "CentOS Stream 8",
    "uid": "a0fcec4766ba4ec7",
    "url": "https://github.com/mlcommons/inference_results_v3.1/tree/master/open/Dell/results/1-node-2S-SPR-PyTorch-INT8/dlrm-v2-99/Offline",
    "version": "v3.1"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Azure/results/ND96amsr_A100_v4_TRT_Triton/dlrm-99/Offline",
    "MlperfModel": "dlrm-99",
    "Model": "dlrm-99",
    "Organization": "Azure",
    "Platform": "ND96amsr_A100_v4_TRT_Triton",
    "Result": 2420440,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "ND96amsr_A100_v4 (8x A100-SXM-80GB, TensorRT, Triton)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 8.4.0, CUDA 11.6",
    "git_url": "https://github.com/mlcommons/inference_results_v2.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 7V12 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 18.04.6 LTS",
    "uid": "f8e2258788174afb",
    "url": "https://github.com/mlcommons/inference_results_v2.0/tree/master/open/Azure/results/ND96amsr_A100_v4_TRT_Triton/dlrm-99/Offline",
    "version": "v2.0"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Azure/results/ND96amsr_A100_v4_TRT_Triton/dlrm-99.9/Offline",
    "MlperfModel": "dlrm-99.9",
    "Model": "dlrm-99.9",
    "Organization": "Azure",
    "Platform": "ND96amsr_A100_v4_TRT_Triton",
    "Result": 2420440,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "ND96amsr_A100_v4 (8x A100-SXM-80GB, TensorRT, Triton)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 8.4.0, CUDA 11.6",
    "git_url": "https://github.com/mlcommons/inference_results_v2.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 7V12 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 18.04.6 LTS",
    "uid": "ab66af1de5c343d2",
    "url": "https://github.com/mlcommons/inference_results_v2.0/tree/master/open/Azure/results/ND96amsr_A100_v4_TRT_Triton/dlrm-99.9/Offline",
    "version": "v2.0"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Azure/results/ND96amsr_A100_v4_TRT/dlrm-99/Offline",
    "MlperfModel": "dlrm-99",
    "Model": "dlrm-99",
    "Organization": "Azure",
    "Platform": "ND96amsr_A100_v4_TRT",
    "Result": 2495550,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "ND96amsr_A100_v4 (8x A100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 8.4.0, CUDA 11.6",
    "git_url": "https://github.com/mlcommons/inference_results_v2.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 7V12 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 18.04.6 LTS",
    "uid": "f4b64ec7aac54275",
    "url": "https://github.com/mlcommons/inference_results_v2.0/tree/master/open/Azure/results/ND96amsr_A100_v4_TRT/dlrm-99/Offline",
    "version": "v2.0"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Azure/results/ND96amsr_A100_v4_TRT/dlrm-99.9/Offline",
    "MlperfModel": "dlrm-99.9",
    "Model": "dlrm-99.9",
    "Organization": "Azure",
    "Platform": "ND96amsr_A100_v4_TRT",
    "Result": 2495550,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "ND96amsr_A100_v4 (8x A100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 8.4.0, CUDA 11.6",
    "git_url": "https://github.com/mlcommons/inference_results_v2.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 7V12 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 18.04.6 LTS",
    "uid": "3953ffcd0e804837",
    "url": "https://github.com/mlcommons/inference_results_v2.0/tree/master/open/Azure/results/ND96amsr_A100_v4_TRT/dlrm-99.9/Offline",
    "version": "v2.0"
  },
  {
    "Accuracy": 80.172,
    "Accuracy_div_100": 0.80172,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Supermicro/results/A100X4_751GE_TRT/dlrm-99/Offline",
    "MlperfModel": "dlrm-99",
    "Model": "dlrm-99",
    "Organization": "Supermicro",
    "Platform": "A100X4_751GE_TRT",
    "Result": 1116970,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "SYS-751GE-TNRT, TensorRT",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.0",
    "git_url": "https://github.com/mlcommons/inference_results_v3.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "INTEL XEON GOLD 6444Y 16-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "585db0404c354081",
    "url": "https://github.com/mlcommons/inference_results_v3.0/tree/master/open/Supermicro/results/A100X4_751GE_TRT/dlrm-99/Offline",
    "version": "v3.0"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Supermicro/results/A100X4_751GE_TRT/dlrm-99.9/Offline",
    "MlperfModel": "dlrm-99.9",
    "Model": "dlrm-99.9",
    "Organization": "Supermicro",
    "Platform": "A100X4_751GE_TRT",
    "Result": 1117560,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "SYS-751GE-TNRT, TensorRT",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.0",
    "git_url": "https://github.com/mlcommons/inference_results_v3.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "INTEL XEON GOLD 6444Y 16-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "54147bc15d484157",
    "url": "https://github.com/mlcommons/inference_results_v3.0/tree/master/open/Supermicro/results/A100X4_751GE_TRT/dlrm-99.9/Offline",
    "version": "v3.0"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Supermicro/results/A100X4_751GE_TRT_Triton/dlrm-99/Offline",
    "MlperfModel": "dlrm-99",
    "Model": "dlrm-99",
    "Organization": "Supermicro",
    "Platform": "A100X4_751GE_TRT_Triton",
    "Result": 1107430,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "SYS-751GE-TNRT, TensorRT, Triton",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.0",
    "git_url": "https://github.com/mlcommons/inference_results_v3.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "INTEL XEON GOLD 6444Y 16-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "ed01576e13d146e4",
    "url": "https://github.com/mlcommons/inference_results_v3.0/tree/master/open/Supermicro/results/A100X4_751GE_TRT_Triton/dlrm-99/Offline",
    "version": "v3.0"
  },
  {
    "Accuracy": 80.185,
    "Accuracy_div_100": 0.80185,
    "Availability": "available",
    "Division": "open",
    "Location": "open/Supermicro/results/A100X4_751GE_TRT_Triton/dlrm-99.9/Offline",
    "MlperfModel": "dlrm-99.9",
    "Model": "dlrm-99.9",
    "Organization": "Supermicro",
    "Platform": "A100X4_751GE_TRT_Triton",
    "Result": 1107430,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "SYS-751GE-TNRT, TensorRT, Triton",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA A100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.0",
    "git_url": "https://github.com/mlcommons/inference_results_v3.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "INTEL XEON GOLD 6444Y 16-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "7e95faf135ea4bc4",
    "url": "https://github.com/mlcommons/inference_results_v3.0/tree/master/open/Supermicro/results/A100X4_751GE_TRT_Triton/dlrm-99.9/Offline",
    "version": "v3.0"
  },
  {
    "Accuracy": 80.491,
    "Accuracy_div_100": 0.80491,
    "Availability": "rdi",
    "Division": "open",
    "Location": "open/Neuchips/results/NEUCHIPS-DLRM-AE01/dlrm-99/Offline",
    "MlperfModel": "dlrm-99",
    "Model": "dlrm-99",
    "Organization": "Neuchips",
    "Platform": "NEUCHIPS-DLRM-AE01",
    "Result": 83402.3,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NEUCHIPS-DLRM-AE01",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 1,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v3.0",
    "has_power": false,
    "host_processor_core_count": "",
    "host_processor_model_name": "AMD EPYC 9004 series",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04",
    "uid": "ec891cc456344046",
    "url": "https://github.com/mlcommons/inference_results_v3.0/tree/master/open/Neuchips/results/NEUCHIPS-DLRM-AE01/dlrm-99/Offline",
    "version": "v3.0"
  },
  {
    "Accuracy": 80.491,
    "Accuracy_div_100": 0.80491,
    "Availability": "rdi",
    "Division": "open",
    "Location": "open/Neuchips/results/NEUCHIPS-DLRM-AE01/dlrm-99.9/Offline",
    "MlperfModel": "dlrm-99.9",
    "Model": "dlrm-99.9",
    "Organization": "Neuchips",
    "Platform": "NEUCHIPS-DLRM-AE01",
    "Result": 83402.3,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NEUCHIPS-DLRM-AE01",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 1,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v3.0",
    "has_power": false,
    "host_processor_core_count": "",
    "host_processor_model_name": "AMD EPYC 9004 series",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04",
    "uid": "c616b790b38a4a0a",
    "url": "https://github.com/mlcommons/inference_results_v3.0/tree/master/open/Neuchips/results/NEUCHIPS-DLRM-AE01/dlrm-99.9/Offline",
    "version": "v3.0"
  }
]
