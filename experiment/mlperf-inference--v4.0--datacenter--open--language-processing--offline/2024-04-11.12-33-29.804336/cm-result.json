[
  {
    "Accuracy": 90.87487229720105,
    "Accuracy_div_100": 0.90875,
    "Availability": "available",
    "Division": "open",
    "Location": "open/CTuning/results/arjun_spr-reference-cpu-pytorch-v2.1.0-default_config/bert-99/offline",
    "MlperfModel": "bert-99",
    "Model": "bert-99",
    "Organization": "CTuning",
    "Platform": "arjun_spr-reference-cpu-pytorch-v2.1.0-default_config",
    "Result": 145.36,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "Network SUT GATE Overflow Intel Sapphire Rapids (1x RTX 4090)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerators_per_node": 1,
    "compliance": 1,
    "errors": 0,
    "framework": "MLCommons python reference implementation with CM API, Onnxruntime v1.16.3",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "Intel(R) Xeon(R) w7-2495X",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v1.6.2. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 23.04 (linux-6.2.0-39-generic-glibc2.37)",
    "uid": "5f8fd7c7ab404d96",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/CTuning/results/arjun_spr-reference-cpu-pytorch-v2.1.0-default_config/bert-99/offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 42.0595,
    "Accuracy_div_100": 0.42059,
    "Availability": "available",
    "Division": "open",
    "Location": "open/CTuning/results/GATE_Overflow_Intel_Sapphire_Rapids-reference-gpu-pytorch-v2.1.1-default_config/llama2-70b-99/offline",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "CTuning",
    "Platform": "GATE_Overflow_Intel_Sapphire_Rapids-reference-gpu-pytorch-v2.1.1-default_config",
    "Result": 199.333,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "GATEOverflow Intel Sapphire Rapids (2x RTX 4090)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerators_per_node": 2,
    "compliance": 1,
    "errors": 0,
    "framework": "MLCommons python reference implementation with CM API, Pytorch v2.1.1",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "Intel(R) Xeon(R) w7-2495X",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v1.6.2. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 23.04 (linux-6.2.0-39-generic-glibc2.37)",
    "uid": "6cf57fc26b5a4dfd",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/CTuning/results/GATE_Overflow_Intel_Sapphire_Rapids-reference-gpu-pytorch-v2.1.1-default_config/llama2-70b-99/offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 90.87487229720105,
    "Accuracy_div_100": 0.90875,
    "Availability": "available",
    "Division": "open",
    "Location": "open/CTuning/results/PCSPECIALIST_AMD_AM5_PC_with_Nvidia_RTX_4090-reference-cpu-pytorch-v2.2.0-network_loadgen/bert-99/offline",
    "MlperfModel": "bert-99",
    "Model": "bert-99",
    "Organization": "CTuning",
    "Platform": "PCSPECIALIST_AMD_AM5_PC_with_Nvidia_RTX_4090-reference-cpu-pytorch-v2.2.0-network_loadgen",
    "Result": 3.0418,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "Network SUT PCSPECIALIST AMD AM5 PC",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 1,
    "errors": 0,
    "framework": "MLCommons python reference implementation with CM API, Pytorch v2.2.0",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "AMD Ryzen 9 7950X 16-Core Processor",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v2.0.0. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 (linux-6.5.0-14-generic-glibc2.35)",
    "uid": "e1185531208646d4",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/CTuning/results/PCSPECIALIST_AMD_AM5_PC_with_Nvidia_RTX_4090-reference-cpu-pytorch-v2.2.0-network_loadgen/bert-99/offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 90.87487229720105,
    "Accuracy_div_100": 0.90875,
    "Availability": "available",
    "Division": "open",
    "Location": "open/CTuning/results/PCSPECIALIST_AMD_AM5_PC_with_Nvidia_RTX_4090-reference-gpu-onnxruntime-v1.16.3-network_loadgen/bert-99/offline",
    "MlperfModel": "bert-99",
    "Model": "bert-99",
    "Organization": "CTuning",
    "Platform": "PCSPECIALIST_AMD_AM5_PC_with_Nvidia_RTX_4090-reference-gpu-onnxruntime-v1.16.3-network_loadgen",
    "Result": 184.308,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "Network SUT PCSPECIALIST AMD AM5 PC (1x RTX 4090)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerators_per_node": 1,
    "compliance": 1,
    "errors": 0,
    "framework": "MLCommons python reference implementation with CM API, Onnxruntime v1.16.3",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "AMD Ryzen 9 7950X 16-Core Processor",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v2.0.0. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 (linux-6.5.0-14-generic-glibc2.35)",
    "uid": "f353a41ed9f747bd",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/CTuning/results/PCSPECIALIST_AMD_AM5_PC_with_Nvidia_RTX_4090-reference-gpu-onnxruntime-v1.16.3-network_loadgen/bert-99/offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 90.97056894987658,
    "Accuracy_div_100": 0.90971,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/bert-99.9/Offline",
    "MlperfModel": "bert-99.9",
    "Model": "bert-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT",
    "Result": 62986.7,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE Cray XD670 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.3.0, CUDA 12.3",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "8fc440b59c3a40b4",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/bert-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 43.0675,
    "Accuracy_div_100": 0.43068,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/gptj-99/Offline",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT",
    "Result": 223.211,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE Cray XD670 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.3.0, CUDA 12.3",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "74bc665efd064f8a",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/gptj-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 43.0675,
    "Accuracy_div_100": 0.43068,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/gptj-99.9/Offline",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT",
    "Result": 223.951,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE Cray XD670 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.3.0, CUDA 12.3",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.3",
    "uid": "7b74858299184b28",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT_TRT/gptj-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 90.35676151551918,
    "Accuracy_div_100": 0.90357,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/bert-99/Offline",
    "MlperfModel": "bert-99",
    "Model": "bert-99",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT",
    "Result": 22620.8,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Gen11 (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.3.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "75f93428a67540af",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/bert-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 90.97056894987658,
    "Accuracy_div_100": 0.90971,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/bert-99.9/Offline",
    "MlperfModel": "bert-99.9",
    "Model": "bert-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT",
    "Result": 19512.3,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Gen11 (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.3.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "218f163f81514f9b",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_ProLiant_DL380a_H100_PCIe_80GBx4_TRT_TRT/bert-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 90.89637138715483,
    "Accuracy_div_100": 0.90896,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/bert-99.9/Offline",
    "MlperfModel": "bert-99.9",
    "Model": "bert-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT",
    "Result": 6874.3,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Gen11 (4x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.0.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 60,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "1b8d8b40c2e5441b",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/bert-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 43.0936,
    "Accuracy_div_100": 0.43094,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/gptj-99/Offline",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT",
    "Result": 49.8357,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Gen11 (4x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.0.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 60,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "e8e6dd4238484ff6",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/gptj-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 43.0936,
    "Accuracy_div_100": 0.43094,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/gptj-99.9/Offline",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT",
    "Result": 49.8357,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "HPE ProLiant DL380a Gen11 (4x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.0.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 60,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "770c28c3665245b4",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx4_TRT/gptj-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 38.7559,
    "Accuracy_div_100": 0.38756,
    "Availability": "available",
    "Division": "open",
    "Location": "open/JuniperNetworks/results/A100-2N-80GB/llama2-70b-99/Offline",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "JuniperNetworks",
    "Platform": "A100-2N-80GB",
    "Result": 3659.41,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "A100-2N-80GB",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "A100-SXM-80GB",
    "accelerators_per_node": 16,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.2.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 7763 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 2,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "4ee464d10bf4441b",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/JuniperNetworks/results/A100-2N-80GB/llama2-70b-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 38.5151,
    "Accuracy_div_100": 0.38515,
    "Availability": "available",
    "Division": "open",
    "Location": "open/JuniperNetworks/results/H100-2N-80GB/llama2-70b-99/Offline",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "JuniperNetworks",
    "Platform": "H100-2N-80GB",
    "Result": 6235.72,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "H100-2N-80GB",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "H100-SXM-80GB",
    "accelerators_per_node": 16,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 9.2.0, CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 112,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480CL",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 2,
    "operating_system": "Ubuntu 22.04.3",
    "uid": "8c8ec7c591a64f33",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/JuniperNetworks/results/H100-2N-80GB/llama2-70b-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 42.3672,
    "Accuracy_div_100": 0.42367,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT_PRUNED/gptj-99/Offline",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "NVIDIA",
    "Platform": "DGX-H100_H100-SXM-80GBx1_TRT_PRUNED",
    "Result": 43.3445,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NVIDIA DGX H100 (1x H100-SXM-80GB)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 1,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.4",
    "uid": "e433b4c12d074c68",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/NVIDIA/results/DGX-H100_H100-SXM-80GBx1_TRT_PRUNED/gptj-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 44.4855,
    "Accuracy_div_100": 0.44486,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_SPARSE/llama2-70b-99/Offline",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "DGX-H100_H100-SXM-80GBx8_TRT_SPARSE",
    "Result": 28931.9,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NVIDIA DGX H100 (8x H100-SXM-80GB)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.4",
    "uid": "9c23687fc88b4197",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_SPARSE/llama2-70b-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 44.4855,
    "Accuracy_div_100": 0.44486,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_SPARSE/llama2-70b-99.9/Offline",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "DGX-H100_H100-SXM-80GBx8_TRT_SPARSE",
    "Result": 28931.9,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NVIDIA DGX H100 (8x H100-SXM-80GB)",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 20.04.4",
    "uid": "5c9cdd8592344685",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT_SPARSE/llama2-70b-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 43.0965,
    "Accuracy_div_100": 0.43096,
    "Availability": "available",
    "Division": "open",
    "Location": "open/RedHat/results/DGX-RedHat-OpenShift/gptj-99/Offline",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "RedHat",
    "Platform": "DGX-RedHat-OpenShift",
    "Result": 138.338,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NVIDIA DGX H100 (8xH100-SXM-80GB, Red Hat OpenShift AI, vLLM(fp16))",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 112,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480CL",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA H100-SXM-80GB. Red Hat OpenShift Container Platform 4.14 + OpenShift AI",
    "number_of_nodes": 1,
    "operating_system": "Red Hat Enterprise Linux CoreOS release 4.14",
    "uid": "a5a79ace7862411f",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/RedHat/results/DGX-RedHat-OpenShift/gptj-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 43.0965,
    "Accuracy_div_100": 0.43096,
    "Availability": "available",
    "Division": "open",
    "Location": "open/RedHat/results/DGX-RedHat-OpenShift/gptj-99.9/Offline",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "RedHat",
    "Platform": "DGX-RedHat-OpenShift",
    "Result": 138.338,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "NVIDIA DGX H100 (8xH100-SXM-80GB, Red Hat OpenShift AI, vLLM(fp16))",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 112,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480CL",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA H100-SXM-80GB. Red Hat OpenShift Container Platform 4.14 + OpenShift AI",
    "number_of_nodes": 1,
    "operating_system": "Red Hat Enterprise Linux CoreOS release 4.14",
    "uid": "a4a0bc241a264ee3",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/RedHat/results/DGX-RedHat-OpenShift/gptj-99.9/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 44.4162,
    "Accuracy_div_100": 0.44416,
    "Availability": "available",
    "Division": "open",
    "Location": "open/RedHat-Supermicro/results/AS-4125GS-TNRT-RedHat-OpenShift-AI/llama2-70b-99/Offline",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "RedHat-Supermicro",
    "Platform": "AS-4125GS-TNRT-RedHat-OpenShift-AI",
    "Result": 3038.78,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "AS-4125GS-TNRT(8xH100-PCIe-80GB, Red Hat OpenShift AI, vLLM(fp16))",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA H100-PCIe-80GB. Red Hat OpenShift Container Platform 4.14 + OpenShift AI",
    "number_of_nodes": 1,
    "operating_system": "Red Hat Enterprise Linux CoreOS release 4.14",
    "uid": "51f2b6f71f1244cd",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/RedHat-Supermicro/results/AS-4125GS-TNRT-RedHat-OpenShift-AI/llama2-70b-99/Offline",
    "version": "v4.0"
  },
  {
    "Accuracy": 44.4162,
    "Accuracy_div_100": 0.44416,
    "Availability": "available",
    "Division": "open",
    "Location": "open/RedHat-Supermicro/results/AS-4125GS-TNRT-RedHat-OpenShift-AI/llama2-70b-99.9/Offline",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "RedHat-Supermicro",
    "Platform": "AS-4125GS-TNRT-RedHat-OpenShift-AI",
    "Result": 3038.78,
    "Result_Units": "Samples/s",
    "Scenario": "Offline",
    "SystemName": "AS-4125GS-TNRT(8xH100-PCIe-80GB, Red Hat OpenShift AI, vLLM(fp16))",
    "SystemType": "datacenter",
    "Units": "Samples/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "CUDA 12.2",
    "git_url": "https://github.com/mlcommons/inference_results_v4.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA H100-PCIe-80GB. Red Hat OpenShift Container Platform 4.14 + OpenShift AI",
    "number_of_nodes": 1,
    "operating_system": "Red Hat Enterprise Linux CoreOS release 4.14",
    "uid": "96c5410af1f04082",
    "url": "https://github.com/mlcommons/inference_results_v4.0/tree/master/open/RedHat-Supermicro/results/AS-4125GS-TNRT-RedHat-OpenShift-AI/llama2-70b-99.9/Offline",
    "version": "v4.0"
  }
]
