[
  {
    "Accuracy": 43.0346,
    "Accuracy_div_100": 0.43035,
    "Availability": "available",
    "Division": "open",
    "Location": "open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 19627.6,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 1,
    "errors": 0,
    "framework": "TensorRT 10.2.0, CUDA 12.4",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "22.04.4 LTS",
    "uid": "f26317f17c024350",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/open/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "version": "v4.1"
  },
  {
    "Accuracy": 44.5491,
    "Accuracy_div_100": 0.44549,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NeuralMagic/results/4xH100-SXM-80GB_vLLM_FP8-reference-cpu-pytorch-v2.3.1-default_config/llama2-70b-99/server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NeuralMagic",
    "Platform": "4xH100-SXM-80GB_vLLM_FP8-reference-cpu-pytorch-v2.3.1-default_config",
    "Result": 1468.24,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR H100 'beaker' (4x H100-SXM-80GB, vLLM, FP8)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "vLLM 0.5.2",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v2.3.4. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 (linux-6.5.0-35-generic-glibc2.35)",
    "uid": "6a3dad2879214418",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/open/NeuralMagic/results/4xH100-SXM-80GB_vLLM_FP8-reference-cpu-pytorch-v2.3.1-default_config/llama2-70b-99/server",
    "version": "v4.1"
  },
  {
    "Accuracy": 44.1657,
    "Accuracy_div_100": 0.44166,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NeuralMagic/results/4xH100-SXM-80GB_vLLM_GPTQ-reference-cpu-pytorch-v2.3.1-default_config/llama2-70b-99/server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NeuralMagic",
    "Platform": "4xH100-SXM-80GB_vLLM_GPTQ-reference-cpu-pytorch-v2.3.1-default_config",
    "Result": 1164.04,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR H100 'beaker' (4x H100-SXM-80GB, vLLM, GPTQ)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 4,
    "compliance": 1,
    "errors": 0,
    "framework": "vLLM 0.5.2",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v2.3.4. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 (linux-6.5.0-35-generic-glibc2.35)",
    "uid": "09115f70eb3e4ae7",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/open/NeuralMagic/results/4xH100-SXM-80GB_vLLM_GPTQ-reference-cpu-pytorch-v2.3.1-default_config/llama2-70b-99/server",
    "version": "v4.1"
  },
  {
    "Accuracy": 42.0442,
    "Accuracy_div_100": 0.42044,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NeuralMagic/results/GO_2xRTX4090-reference-cpu-pytorch-v2.2.1-default_config/neuralmagic_Llama-2-7b-chat-hf-FP8/server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "neuralmagic_Llama-2-7b-chat-hf-FP8",
    "Organization": "NeuralMagic",
    "Platform": "GO_2xRTX4090-reference-cpu-pytorch-v2.2.1-default_config",
    "Result": 1954.36,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "GATE Overflow Intel Sapphire Rapids RTX 4090 (2x RTX 4090, vLLM, FP8)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerators_per_node": 2,
    "compliance": 1,
    "errors": 0,
    "framework": "vLLM 0.5.2",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "Intel(R) Xeon(R) w7-2495X",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v2.3.1. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 23.04 (linux-6.2.0-39-generic-glibc2.37)",
    "uid": "e8023ee4c1484dcf",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/open/NeuralMagic/results/GO_2xRTX4090-reference-cpu-pytorch-v2.2.1-default_config/neuralmagic_Llama-2-7b-chat-hf-FP8/server",
    "version": "v4.1"
  },
  {
    "Accuracy": 42.0303,
    "Accuracy_div_100": 0.4203,
    "Availability": "available",
    "Division": "open",
    "Location": "open/NeuralMagic/results/pcspecialist_amd_am5-reference-gpu-pytorch-v2.2.1-default_config/neuralmagic_Llama-2-7b-chat-hf-FP8/server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "neuralmagic_Llama-2-7b-chat-hf-FP8",
    "Organization": "NeuralMagic",
    "Platform": "pcspecialist_amd_am5-reference-gpu-pytorch-v2.2.1-default_config",
    "Result": 1429.85,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "PCSPECIALIST AMD AM5 (1x RTX 4090)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GeForce RTX 4090",
    "accelerators_per_node": 1,
    "compliance": 1,
    "errors": 0,
    "framework": "deepsparse v1.5.2",
    "git_url": "https://github.com/mlcommons/submissions_inference_v4.1",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "AMD Ryzen 9 7950X 16-Core Processor",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "Automated by MLCommons CM v2.3.1. ",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 (linux-6.5.0-41-generic-glibc2.35)",
    "uid": "b82b6d0c5ed64f97",
    "url": "https://github.com/mlcommons/submissions_inference_v4.1/tree/master/open/NeuralMagic/results/pcspecialist_amd_am5-reference-gpu-pytorch-v2.2.1-default_config/neuralmagic_Llama-2-7b-chat-hf-FP8/server",
    "version": "v4.1"
  }
]
