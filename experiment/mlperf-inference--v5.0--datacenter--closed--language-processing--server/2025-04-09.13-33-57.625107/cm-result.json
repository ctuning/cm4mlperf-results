[
  {
    "Accuracy": "ROUGE1: 44.4297  ROUGE2: 22.0409  ROUGEL: 28.6367  TOKENS_PER_SAMPLE: 295.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 30457.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "ff25bfd54a894a07",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 19363.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "a316d93648054c4e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 19363.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "f96226eed93f40bf",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5336  ROUGE2: 22.0916  ROUGEL: 28.7043  TOKENS_PER_SAMPLE: 294.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 14008.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "7a48ed3d5e8f4cf2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5144  ROUGE2: 22.0732  ROUGEL: 28.686  TOKENS_PER_SAMPLE: 294.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 13987.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "72cba5639dad4e49",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4184  ROUGE2: 22.033  ROUGEL: 28.6265  TOKENS_PER_SAMPLE: 295.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 30500.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "5dae807406434239",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5621  ROUGE2: 22.1437  ROUGEL: 28.7928  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 27638.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "35b9012ba1ba40ed",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 18065.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "f532038d8f38475d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 18065.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "28036776d4b34726",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5621  ROUGE2: 22.1437  ROUGEL: 28.7928  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 27638.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "6e5ccb4924ee498d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4303  ROUGE2: 22.039  ROUGEL: 28.6356  TOKENS_PER_SAMPLE: 295.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 30498.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "4dba99fd6c184b75",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 20795.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "6e6b06b50d1b453a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 20991.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "705365e1461c42d1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5203  ROUGE2: 22.0752  ROUGEL: 28.6917  TOKENS_PER_SAMPLE: 294.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 14004.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "99790b6be84b4d6f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5263  ROUGE2: 22.0802  ROUGEL: 28.6949  TOKENS_PER_SAMPLE: 294.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 14014.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "aaf58f1d2a2e477d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4347  ROUGE2: 22.0423  ROUGEL: 28.6376  TOKENS_PER_SAMPLE: 295.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_TRT",
    "Result": 30480.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "98adc28839574344",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5208  ROUGE2: 22.0821  ROUGEL: 28.7224  TOKENS_PER_SAMPLE: 291.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0",
    "Result": 3181.65,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 9.0, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Llama2-70b used MLPerf v4.1 TensorRT-LLM instead",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "82367d49ef8140b7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp16"
  },
  {
    "Accuracy": "ROUGE1: 44.5208  ROUGE2: 22.0821  ROUGEL: 28.7224  TOKENS_PER_SAMPLE: 291.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0",
    "Result": 3181.65,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 9.0, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Llama2-70b used MLPerf v4.1 TensorRT-LLM instead",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "e91569bbf15e4637",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT_9.0/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp16"
  },
  {
    "Accuracy": "ROUGE1: 44.5531  ROUGE2: 22.1392  ROUGEL: 28.7903  TOKENS_PER_SAMPLE: 291.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 4327.68,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "4996528e70fd4a2b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1186  ROUGE2: 20.1722  ROUGEL: 30.0185  GEN_LEN: 4156655",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 2526.03,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "cab4788d25ff4774",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1186  ROUGE2: 20.1722  ROUGEL: 30.0185  GEN_LEN: 4156655",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 2526.03,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "6c5d310c016242e0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5531  ROUGE2: 22.1392  ROUGEL: 28.7903  TOKENS_PER_SAMPLE: 291.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 4327.68,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "74f1434072ea4908",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1396  ROUGE2: 20.1904  ROUGEL: 30.0525  GEN_LEN: 4153772",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT",
    "Result": 6821.11,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "a2301b7b67374c7a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1396  ROUGE2: 20.1904  ROUGEL: 30.0525  GEN_LEN: 4153772",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT",
    "Result": 6821.11,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "b1bbfd0665a94adf",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0194  ROUGE2: 20.0851  ROUGEL: 29.9991  GEN_LEN: 4155554",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_Hawks_3200_H100_NVL_94GBx4_TRT",
    "Result": 7179.86,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Compute Scale-up Server 3200 (4x H100-NVL-94GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8468",
    "host_processors_per_node": 4,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "b6f76bd7a1744744",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Hawks_3200_H100_NVL_94GBx4_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5663  ROUGE2: 22.1467  ROUGEL: 28.7958  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 31281.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "e344ac8c7f004c39",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 21019.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "d6c91c2de8a94e53",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 21025.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "d9f367a096864f68",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.497  ROUGE2: 22.0948  ROUGEL: 28.7299  TOKENS_PER_SAMPLE: 292.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 19405.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "f8875f2676554343",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.497  ROUGE2: 22.0948  ROUGEL: 28.7299  TOKENS_PER_SAMPLE: 292.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 19405.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "e1a58f2f7f0c4bcb",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5588  ROUGE2: 22.1412  ROUGEL: 28.7922  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 31283.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "13394e4af5e44de4",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5503  ROUGE2: 22.1345  ROUGEL: 28.7855  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 8674.58,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "138003ce1b8e4424",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1186  ROUGE2: 20.1722  ROUGEL: 30.0185  GEN_LEN: 4156655",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 5039.82,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "f47c53cfbb834504",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1186  ROUGE2: 20.1722  ROUGEL: 30.0185  GEN_LEN: 4156655",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 5039.82,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "76e9ee9218a1472b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5503  ROUGE2: 22.1345  ROUGEL: 28.7855  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 8674.58,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "31a22b492b9a4432",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5513  ROUGE2: 22.1359  ROUGEL: 28.7885  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Sustainable_Metal_Cloud",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33027.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "9d5ab85b70cb4b17",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Sustainable_Metal_Cloud",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21359.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "42cb5738f7ad46fd",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Sustainable_Metal_Cloud",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21359.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "ea1177eb6bc84965",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.7884  ROUGE2: 22.3874  ROUGEL: 29.148  TOKENS_PER_SAMPLE: 273.8",
    "Availability": "preview",
    "Division": "closed",
    "Location": "closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Google",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 92338.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Debian 12",
    "uid": "9852eae2df264f25",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.7895  ROUGE2: 22.3823  ROUGEL: 29.1462  TOKENS_PER_SAMPLE: 273.8",
    "Availability": "preview",
    "Division": "closed",
    "Location": "closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Google",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 92203.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Debian 12",
    "uid": "6d924cae9236439e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.5593  ROUGE2: 22.1417  ROUGEL: 28.7894  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 31390.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "258ac60a40654a31",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 20501.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "15ff13ba46ed428e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 20501.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "c5b29d2a00ac4ec1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5593  ROUGE2: 22.1417  ROUGEL: 28.7894  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 31390.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "85b1eb1f206d45b0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4557  ROUGE2: 22.0578  ROUGEL: 28.672  TOKENS_PER_SAMPLE: 295.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "MangoBoost",
    "Platform": "32xMI300X_2xEPYC_9534",
    "Result": 93039.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "MangoBoost Mi300X (32x MI300X-192GB-HBM3, LLMBoost)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI300X 192GB HBM3",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "ROCm 6.3.0",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9534",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 4,
    "operating_system": "Ubuntu 22.04.5 LTS (jammy)",
    "uid": "37b99f9878484769",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4557  ROUGE2: 22.0578  ROUGEL: 28.672  TOKENS_PER_SAMPLE: 295.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "MangoBoost",
    "Platform": "32xMI300X_2xEPYC_9534",
    "Result": 93039.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "MangoBoost Mi300X (32x MI300X-192GB-HBM3, LLMBoost)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI300X 192GB HBM3",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "ROCm 6.3.0",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9534",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 4,
    "operating_system": "Ubuntu 22.04.5 LTS (jammy)",
    "uid": "f321595171344961",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/MangoBoost/results/32xMI300X_2xEPYC_9534/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5019  ROUGE2: 22.0906  ROUGEL: 28.7528  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 3203.87,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "2319392a5f7e43be",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1186  ROUGE2: 20.1722  ROUGEL: 30.0185  GEN_LEN: 4156655",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 2824.55,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "fad33fa2dda34355",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1186  ROUGE2: 20.1722  ROUGEL: 30.0185  GEN_LEN: 4156655",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 2824.55,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "ba96d92cf1ab41fc",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5019  ROUGE2: 22.0906  ROUGEL: 28.7528  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 3203.87,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "344d3e13941249c0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5665  ROUGE2: 22.1466  ROUGEL: 28.7638  TOKENS_PER_SAMPLE: 292.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 29916,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "aabc19ba819e4cfe",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 19316.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "5ef0a8398fa64412",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 19316.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "9c41edd36c7f45ec",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5665  ROUGE2: 22.1466  ROUGEL: 28.7638  TOKENS_PER_SAMPLE: 292.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 29916,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "0dd2a0590b9747af",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0139  ROUGE2: 20.1165  ROUGEL: 29.9939  GEN_LEN: 4051023",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "1-node-2S-GNR_86C",
    "Result": 168.666,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_86C",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "INTEL(R) XEON(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid-D55X-1U. N/A",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.4",
    "uid": "d4f9198225c94108",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "INT8"
  },
  {
    "Accuracy": "ROUGE1: 44.6283  ROUGE2: 22.2205  ROUGEL: 28.8297  TOKENS_PER_SAMPLE: 290.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 4889.92,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "5c2742da2d61401d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1385  ROUGE2: 20.1564  ROUGEL: 30.0269  GEN_LEN: 4155440",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 6251.27,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "91d7606871b740f5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1385  ROUGE2: 20.1564  ROUGEL: 30.0269  GEN_LEN: 4155440",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 6251.27,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "771ec19bdb3249c1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.6283  ROUGE2: 22.2205  ROUGEL: 28.8297  TOKENS_PER_SAMPLE: 290.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 4889.92,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "cb748f0a3c34457d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.7469  ROUGE2: 22.3525  ROUGEL: 29.1541  TOKENS_PER_SAMPLE: 272.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx1_TRT",
    "Result": 12078.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (1x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "7c0e39cef086408a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.7469  ROUGE2: 22.3525  ROUGEL: 29.1541  TOKENS_PER_SAMPLE: 272.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx1_TRT",
    "Result": 12078.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (1x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "25a6c093eea44adb",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx1_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.5683  ROUGE2: 22.1486  ROUGEL: 28.7643  TOKENS_PER_SAMPLE: 292.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "DGX-H100_H100-SXM-80GBx8_TRT",
    "Result": 31106.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H100 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b732367c700d4ad5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5683  ROUGE2: 22.1486  ROUGEL: 28.7643  TOKENS_PER_SAMPLE: 292.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "DGX-H100_H100-SXM-80GBx8_TRT",
    "Result": 31106.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX H100 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H100 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "45064e832a1d4508",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/DGX-H100_H100-SXM-80GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5529  ROUGE2: 22.1393  ROUGEL: 28.7909  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 4350.26,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "1fbf7f7e43d944f2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 2526.75,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "16d1ad1c5a2a4160",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 2526.75,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "c5d1cf60273d450a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5529  ROUGE2: 22.1393  ROUGEL: 28.7909  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 4350.26,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d44f85f5bfa04d99",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.7462  ROUGE2: 22.3556  ROUGEL: 29.1559  TOKENS_PER_SAMPLE: 272.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 98443.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0ec2e34aa2554e26",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.827  ROUGE2: 22.4067  ROUGEL: 29.208  TOKENS_PER_SAMPLE: 272.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 59409.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "44325be4fd2846d7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.827  ROUGE2: 22.4067  ROUGEL: 29.208  TOKENS_PER_SAMPLE: 272.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 59409.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e2b96731233140be",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.7462  ROUGE2: 22.3556  ROUGEL: 29.1559  TOKENS_PER_SAMPLE: 272.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "B200-SXM-180GBx8_TRT",
    "Result": 98443.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8570",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "ab6d6ae2242d4199",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.5568  ROUGE2: 22.1417  ROUGEL: 28.7946  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT_Triton",
    "Result": 33024.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT, Triton)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "6e412c4945c54388",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5568  ROUGE2: 22.1417  ROUGEL: 28.7946  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT_Triton",
    "Result": 33024.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT, Triton)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "9831e72e665a487f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT_Triton/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5631  ROUGE2: 22.1489  ROUGEL: 28.797  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33071.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "baa1d439ce7f4485",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21544.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "da9871c3a6124c01",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21544.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "98c3935bae3040ce",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5031  ROUGE2: 22.1008  ROUGEL: 28.7296  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19424.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "ea96df9254664327",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5031  ROUGE2: 22.1008  ROUGEL: 28.7296  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19424.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d02f9227dd4e44b8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5631  ROUGE2: 22.1489  ROUGEL: 28.797  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33071.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e83ff636590d47a7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4508  ROUGE2: 22.058  ROUGEL: 28.6575  TOKENS_PER_SAMPLE: 295.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "AMD",
    "Platform": "8xMI325X_2xEPYC_9575F",
    "Result": 30724.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74A-7U",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "43e17115763042fa",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4508  ROUGE2: 22.058  ROUGEL: 28.6575  TOKENS_PER_SAMPLE: 295.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "AMD",
    "Platform": "8xMI325X_2xEPYC_9575F",
    "Result": 30724.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74A-7U",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "fbcc04d054164197",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/AMD/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5513  ROUGE2: 22.1381  ROUGEL: 28.7884  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Oracle",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33001,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LT",
    "uid": "cc7fc3aa15334468",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Oracle",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21527.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LT",
    "uid": "b1613f825ad04529",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Oracle",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21527.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LT",
    "uid": "f61f63b67a614ad5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Oracle/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5513  ROUGE2: 22.1381  ROUGEL: 28.7884  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Oracle",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33001,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LT",
    "uid": "8194e830d37b48ff",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Oracle/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.3971  ROUGE2: 22.0537  ROUGEL: 28.6495  TOKENS_PER_SAMPLE: 295.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "ASUSTeK",
    "Platform": "ESC_A8A_MI325X_256GBx8",
    "Result": 30086.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC A8A-E12U (8x MI325x-256GB )",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "2x AMD EPYC 9475F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "22.04.5 LTS (Jammy Jellyfish)",
    "uid": "66d4fb798cff40df",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "Dummy"
  },
  {
    "Accuracy": "ROUGE1: 44.3971  ROUGE2: 22.0537  ROUGEL: 28.6495  TOKENS_PER_SAMPLE: 295.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "ASUSTeK",
    "Platform": "ESC_A8A_MI325X_256GBx8",
    "Result": 30086.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC A8A-E12U (8x MI325x-256GB )",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "2x AMD EPYC 9475F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "22.04.5 LTS (Jammy Jellyfish)",
    "uid": "8c93d7d8ef504a12",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "Dummy"
  },
  {
    "Accuracy": "ROUGE1: 44.5628  ROUGE2: 22.1477  ROUGEL: 28.7948  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "ASUSTeK",
    "Platform": "ESC8000_H200_NVLx8_NVLink_TRT",
    "Result": 27417.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9475F 48-Core Processo",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "da75ebe4304f4913",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1301  ROUGE2: 20.133  ROUGEL: 30.0141  GEN_LEN: 4157005",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "ASUSTeK",
    "Platform": "ESC8000_H200_NVLx8_NVLink_TRT",
    "Result": 17953.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9475F 48-Core Processo",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4aa6dbd8b50646c0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1301  ROUGE2: 20.133  ROUGEL: 30.0141  GEN_LEN: 4157005",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "ASUSTeK",
    "Platform": "ESC8000_H200_NVLx8_NVLink_TRT",
    "Result": 17953.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9475F 48-Core Processo",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "6875fdcb040f4953",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5628  ROUGE2: 22.1477  ROUGEL: 28.7948  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "ASUSTeK",
    "Platform": "ESC8000_H200_NVLx8_NVLink_TRT",
    "Result": 27417.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9475F 48-Core Processo",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "9ad7166812994fe9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5621  ROUGE2: 22.1419  ROUGEL: 28.7948  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33037.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e519586824244f0d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1301  ROUGE2: 20.133  ROUGEL: 30.0141  GEN_LEN: 4157005",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21533.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "a4b67679e0004d45",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1301  ROUGE2: 20.133  ROUGEL: 30.0141  GEN_LEN: 4157005",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21533.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "36a6856fd3854ae5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5018  ROUGE2: 22.0993  ROUGEL: 28.7329  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19418.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "bb5fc4859b284ad8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5018  ROUGE2: 22.0993  ROUGEL: 28.7329  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19418.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "5e3212bfdc6249b7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5621  ROUGE2: 22.1419  ROUGEL: 28.7948  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "ASUSTeK",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33037.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC N8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8558",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "384e4fe2593745f7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Broadcom_Supermicro",
    "Platform": "vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT",
    "Result": 19318.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": " SYS-821GE-TNRT (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "Virtualized NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "VMware ESXi, 8.0.3, 24501827 and vGPU_17.4_GA_AIE_ESXi_Host_Drivers for vGPUs",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e6be251e23844b24",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Broadcom_Supermicro",
    "Platform": "vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT",
    "Result": 19318.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": " SYS-821GE-TNRT (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "Virtualized NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "VMware ESXi, 8.0.3, 24501827 and vGPU_17.4_GA_AIE_ESXi_Host_Drivers for vGPUs",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "7783871055304e4e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Broadcom_Supermicro/results/vSYS_821GE_TNRT_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4157  ROUGE2: 22.0162  ROUGEL: 28.6494  TOKENS_PER_SAMPLE: 296.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "GigaComputing",
    "Platform": "G893-ZX1-AAX2",
    "Result": 29094.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-ZX1-AAX2",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "ROCm 6.3.3",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "2xAMD EPY 9175F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5",
    "uid": "b00ad3a6844145fb",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.4157  ROUGE2: 22.0162  ROUGEL: 28.6494  TOKENS_PER_SAMPLE: 296.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "GigaComputing",
    "Platform": "G893-ZX1-AAX2",
    "Result": 29094.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-ZX1-AAX2",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "ROCm 6.3.3",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 16,
    "host_processor_model_name": "2xAMD EPY 9175F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5",
    "uid": "25a40ae165d64162",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-ZX1-AAX2/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5553  ROUGE2: 22.1419  ROUGEL: 28.79  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 33054,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "3116c0a76f6e456e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 21544.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "3f2d50d58bf4434f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 21544.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "f204aac124184c1b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5085  ROUGE2: 22.1058  ROUGEL: 28.7374  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 20234.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "33edf591c036473f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5085  ROUGE2: 22.1058  ROUGEL: 28.7374  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 20234.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "6483dbc8cc7a466c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5553  ROUGE2: 22.1419  ROUGEL: 28.79  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 33054,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "0c85ece084f745c5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 42.9548  ROUGE2: 20.1209  ROUGEL: 29.9662  GEN_LEN: 4000345",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Intel/results/1-node-2S-GNR_128C/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Intel",
    "Platform": "1-node-2S-GNR_128C",
    "Result": 265.055,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_128C",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "INTEL(R) XEON(R) 6980P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "N/A. N/A",
    "number_of_nodes": 1,
    "operating_system": "CentOS Stream 9",
    "uid": "ca053ebf995c49ce",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Intel/results/1-node-2S-GNR_128C/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "INT8"
  },
  {
    "Accuracy": "ROUGE1: 42.9548  ROUGE2: 20.1209  ROUGEL: 29.9662  GEN_LEN: 4000345",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Intel/results/1-node-2S-GNR_128C/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Intel",
    "Platform": "1-node-2S-GNR_128C",
    "Result": 265.055,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_128C",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "INTEL(R) XEON(R) 6980P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "N/A. N/A",
    "number_of_nodes": 1,
    "operating_system": "CentOS Stream 9",
    "uid": "1589b587a6c74ca2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Intel/results/1-node-2S-GNR_128C/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "INT8"
  },
  {
    "Accuracy": "ROUGE1: 44.5641  ROUGE2: 22.1454  ROUGEL: 28.794  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "coreweave",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33066.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "9760a9cb746e4bde",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5641  ROUGE2: 22.1454  ROUGEL: 28.794  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "coreweave",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33066.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e47ef2aaa43648df",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/coreweave/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.6229  ROUGE2: 22.2108  ROUGEL: 28.8257  TOKENS_PER_SAMPLE: 291.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Cisco",
    "Platform": "C885A_H100_SXMx8_TRT",
    "Result": 26599.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d47d36d302f14df3",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Cisco",
    "Platform": "C885A_H100_SXMx8_TRT",
    "Result": 20706.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "f817e980411c49d9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Cisco",
    "Platform": "C885A_H100_SXMx8_TRT",
    "Result": 20706.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "1847e89d532d4e5c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_H100_SXMx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.6229  ROUGE2: 22.2108  ROUGEL: 28.8257  TOKENS_PER_SAMPLE: 291.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Cisco",
    "Platform": "C885A_H100_SXMx8_TRT",
    "Result": 26599.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9554 64-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b7a3e72a20884719",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_H100_SXMx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0249  ROUGE2: 20.0689  ROUGEL: 29.9271  GEN_LEN: 4153891",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/X215M8-L40Sx2_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Cisco",
    "Platform": "X215M8-L40Sx2_TRT",
    "Result": 1745.78,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "UCS X215 M8 (2x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9224 24-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "50a9b3b703064df8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/X215M8-L40Sx2_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 42.999  ROUGE2: 20.1137  ROUGEL: 29.9827  GEN_LEN: 4048975",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/1-node-2S-GNR_86C/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Cisco",
    "Platform": "1-node-2S-GNR_86C",
    "Result": 168.688,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_86C",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "INTEL(R) XEON(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Cisco UCS C240 M8. N/A",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4 LTS",
    "uid": "bab2723c1b104811",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/1-node-2S-GNR_86C/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "INT8"
  },
  {
    "Accuracy": "ROUGE1: 44.5103  ROUGE2: 22.1066  ROUGEL: 28.7387  TOKENS_PER_SAMPLE: 292.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Cisco",
    "Platform": "C245M8_H100NVL_94GBx2_TRT",
    "Result": 3337.55,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C245 M8 (2x H100NVL-PCIe-94GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "AMD EPYC 9224",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "9f6a3e8b61b54a83",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Cisco",
    "Platform": "C245M8_H100NVL_94GBx2_TRT",
    "Result": 3305.27,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C245 M8 (2x H100NVL-PCIe-94GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "AMD EPYC 9224",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "47206cc0baf54dd6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5111  ROUGE2: 22.1063  ROUGEL: 28.7376  TOKENS_PER_SAMPLE: 292.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/X215M8_H100NVLx2_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Cisco",
    "Platform": "X215M8_H100NVLx2_TRT",
    "Result": 3337.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS X215c M8 with X440P PCIe node (2x H100NVL-PCIe-94GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "AMD EPYC 9554",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "8cbb6a5f763542d0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/X215M8_H100NVLx2_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/X215M8_H100NVLx2_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Cisco",
    "Platform": "X215M8_H100NVLx2_TRT",
    "Result": 3701.78,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS X215c M8 with X440P PCIe node (2x H100NVL-PCIe-94GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "AMD EPYC 9554",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "fe2c12215dc44fc9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/X215M8_H100NVLx2_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5122  ROUGE2: 22.1039  ROUGEL: 28.7358  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Cisco",
    "Platform": "C885A_M8_H200_SXM_141GBx8_TRT",
    "Result": 30420.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b94bf50f777346e5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Cisco",
    "Platform": "C885A_M8_H200_SXM_141GBx8_TRT",
    "Result": 21813.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "26b1d4f7208a40ed",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Cisco",
    "Platform": "C885A_M8_H200_SXM_141GBx8_TRT",
    "Result": 21813.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "5af98e5080cf40fb",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5023  ROUGE2: 22.0975  ROUGEL: 28.7294  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Cisco",
    "Platform": "C885A_M8_H200_SXM_141GBx8_TRT",
    "Result": 19437.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "be2fe763be48411d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5122  ROUGE2: 22.1039  ROUGEL: 28.7358  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Cisco",
    "Platform": "C885A_M8_H200_SXM_141GBx8_TRT",
    "Result": 30420.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d55a4ff51c16416d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C885A_M8_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5554  ROUGE2: 22.138  ROUGEL: 28.7885  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33045.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "f9680da3483e4a49",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21543.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "07aa6f6a94f2443a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21543.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4946d20421a143a0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5019  ROUGE2: 22.0977  ROUGEL: 28.7293  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19381,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "47292184b8c84263",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5019  ROUGE2: 22.0977  ROUGEL: 28.7293  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19381,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "db113e7e74694880",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5554  ROUGE2: 22.138  ROUGEL: 28.7885  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 33045.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b26502c2a4364cd8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.7857  ROUGE2: 22.3844  ROUGEL: 29.1435  TOKENS_PER_SAMPLE: 273.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Supermicro",
    "Platform": "SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT",
    "Result": 97552.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "9e5f79089da9443e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.8117  ROUGE2: 22.3683  ROUGEL: 29.1731  TOKENS_PER_SAMPLE: 272.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Supermicro",
    "Platform": "SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT",
    "Result": 62265.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "0a25b97e59224029",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.8117  ROUGE2: 22.3683  ROUGEL: 29.1731  TOKENS_PER_SAMPLE: 272.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT",
    "Result": 62265.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "13f15b3faecf4266",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.7857  ROUGE2: 22.3844  ROUGEL: 29.1435  TOKENS_PER_SAMPLE: 273.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT",
    "Result": 97552.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "8bc3766c7e5f4952",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_A21GE_NBRT_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.5615  ROUGE2: 22.1445  ROUGEL: 28.7927  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 33052.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "7a67625c8b184e14",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 21544.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "67985ca56e0a4fa6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 21544.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "f34a01cb0d204778",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5006  ROUGE2: 22.1008  ROUGEL: 28.7314  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 19431.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "86609d569b174a64",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5006  ROUGE2: 22.1008  ROUGEL: 28.7314  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 19431.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4aa7e598eee84eee",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5615  ROUGE2: 22.1445  ROUGEL: 28.7927  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_821GE_TNHR_H200_SXM_141GBX8_TRT",
    "Result": 33052.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-821GE-TNHR (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "2daefe615a45448d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_821GE_TNHR_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5599  ROUGE2: 22.1445  ROUGEL: 28.7952  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 33045.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "9f621d4031014ec1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 21757.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d41d597eb16e43f2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 21757.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "20bd1fd0b38c4910",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5031  ROUGE2: 22.1063  ROUGEL: 28.7341  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 19413.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "8114e7f14cab4f47",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5031  ROUGE2: 22.1063  ROUGEL: 28.7341  TOKENS_PER_SAMPLE: 292.5",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 19413.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0151918802914c2b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5599  ROUGE2: 22.1445  ROUGEL: 28.7952  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Supermicro",
    "Platform": "AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT",
    "Result": 33045.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-4125GS-TNHR2-LCC (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9654",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "f64ec52615714eae",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/AS_4125GS_TNHR2_LCC_H200_SXM_141GBX8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.7848  ROUGE2: 22.3844  ROUGEL: 29.1432  TOKENS_PER_SAMPLE: 273.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT",
    "Result": 97629.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "a576310394384684",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.8002  ROUGE2: 22.3621  ROUGEL: 29.1653  TOKENS_PER_SAMPLE: 272.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT",
    "Result": 59505.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "aa8530f963fb4ae9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.8002  ROUGE2: 22.3621  ROUGEL: 29.1653  TOKENS_PER_SAMPLE: 272.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT",
    "Result": 59505.3,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "9c3423299cbe4fda",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 44.7848  ROUGE2: 22.3844  ROUGEL: 29.1432  TOKENS_PER_SAMPLE: 273.8",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT",
    "Result": 97629.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-421GE-NBRT-LCC (8x B200-SXM-180GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA B200-SXM-180GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "B200 TGP 1000W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.5 LTS",
    "uid": "7c7b9f08a7774cf7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS_421GE_NBRT_LCC_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp4"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Supermicro",
    "Platform": "SYS522GA_H200X8_NVL_TRT",
    "Result": 18571.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-522GA-NRT(8x H200-NVL-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "Intel(R) Xeon(R) 6979P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "ae15a44610024a11",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Supermicro",
    "Platform": "SYS522GA_H200X8_NVL_TRT",
    "Result": 18571.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-522GA-NRT(8x H200-NVL-141GB)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "Intel(R) Xeon(R) 6979P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "289d007fa9bf45bc",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/SYS522GA_H200X8_NVL_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.029  ROUGE2: 20.1443  ROUGEL: 29.9941  GEN_LEN: 4028137",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/1-node-2S-GNR_128C/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Supermicro",
    "Platform": "1-node-2S-GNR_128C",
    "Result": 251.31,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "SYS-822GA-NGR3",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "INTEL(R) XEON(R) 6980P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "N/A",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04",
    "uid": "9db68df077a54286",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/1-node-2S-GNR_128C/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "INT8"
  },
  {
    "Accuracy": "ROUGE1: 44.46  ROUGE2: 22.0494  ROUGEL: 28.6787  TOKENS_PER_SAMPLE: 295.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Supermicro",
    "Platform": "8xMI325X_2xEPYC_9575F",
    "Result": 28770.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-8126GS-TNMR",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "vLLM 0.0.0.dev1+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 LTS",
    "uid": "2271784364224cde",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "Dummy"
  },
  {
    "Accuracy": "ROUGE1: 44.46  ROUGE2: 22.0494  ROUGEL: 28.6787  TOKENS_PER_SAMPLE: 295.2",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Supermicro",
    "Platform": "8xMI325X_2xEPYC_9575F",
    "Result": 28770.2,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "AS-8126GS-TNMR",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "AMD Instinct MI325X 256GB HBM3E",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "vLLM 0.0.0.dev1+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9575F",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04 LTS",
    "uid": "b17ffba142b1498e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "Dummy"
  },
  {
    "Accuracy": "ROUGE1: 44.5272  ROUGE2: 22.1154  ROUGEL: 28.7622  TOKENS_PER_SAMPLE: 291.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Lenovo",
    "Platform": "SR675v3_H200_SXMx4_TRT",
    "Result": 14652.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR675 V3 (4x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9655 96-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e8b60123b1354861",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Lenovo",
    "Platform": "SR675v3_H200_SXMx4_TRT",
    "Result": 10503.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR675 V3 (4x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9655 96-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "27ee1107c3ed4b91",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Lenovo",
    "Platform": "SR675v3_H200_SXMx4_TRT",
    "Result": 10503.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR675 V3 (4x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9655 96-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "ac4b641de7204a84",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5272  ROUGE2: 22.1154  ROUGEL: 28.7622  TOKENS_PER_SAMPLE: 291.7",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Lenovo",
    "Platform": "SR675v3_H200_SXMx4_TRT",
    "Result": 14652.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR675 V3 (4x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "AMD EPYC 9655 96-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "48ff3fb5eaa54186",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR675v3_H200_SXMx4_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Lenovo",
    "Platform": "H100-NVL-94GBx4_TRT",
    "Result": 6773.87,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR650a V4 (4x H100-NVL-94GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "Intel(R) Xeon(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d602392a554c4614",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Lenovo",
    "Platform": "H100-NVL-94GBx4_TRT",
    "Result": 6773.87,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR650a V4 (4x H100-NVL-94GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "Intel(R) Xeon(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "a6bf0a86a23d475b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H100-NVL-94GBx4_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5214  ROUGE2: 22.1163  ROUGEL: 28.7576  TOKENS_PER_SAMPLE: 291.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 32934.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "7643eb2b1adc4cd1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 21553.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "e35bbff179a74163",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 21553.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0f4208eab2184908",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.576  ROUGE2: 22.1459  ROUGEL: 28.7741  TOKENS_PER_SAMPLE: 292.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 19448.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4e9759df60894c80",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.576  ROUGE2: 22.1459  ROUGEL: 28.7741  TOKENS_PER_SAMPLE: 292.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 19448.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "6662e583010b4fe5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5214  ROUGE2: 22.1163  ROUGEL: 28.7576  TOKENS_PER_SAMPLE: 291.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Lenovo",
    "Platform": "SR780a_V3_H200SXMx8",
    "Result": 32934.7,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR780a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "5da81473404c4c62",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/SR780a_V3_H200SXMx8/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5225  ROUGE2: 22.1121  ROUGEL: 28.7551  TOKENS_PER_SAMPLE: 291.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 32976.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "014a9abf3ab94684",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21549.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0414dbd1eb394d3b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0858  ROUGE2: 20.1151  ROUGEL: 29.9685  GEN_LEN: 4153033",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 21549.1,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0ce3ec5f5b6e4152",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5762  ROUGE2: 22.1484  ROUGEL: 28.7736  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19492.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "534e20017aca4322",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5762  ROUGE2: 22.1484  ROUGEL: 28.7736  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 19492.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b1739ae47a9b4bb5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5225  ROUGE2: 22.1121  ROUGEL: 28.7551  TOKENS_PER_SAMPLE: 291.6",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Lenovo",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 32976.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "ThinkSystem SR680a V3(8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d831f584995d4037",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lenovo/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5602  ROUGE2: 22.142  ROUGEL: 28.794  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 33023.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4d9e3ea691f3426e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 21455.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "7f8b1ea68d9a4787",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 21455.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d12b7cc90af9448f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5133  ROUGE2: 22.1131  ROUGEL: 28.7404  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 19415.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "711245a1092f4a09",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5133  ROUGE2: 22.1131  ROUGEL: 28.7404  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 19415.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4c17c81c7ba74c2b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5602  ROUGE2: 22.142  ROUGEL: 28.794  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Dell",
    "Platform": "XE9680L_H200_SXM_141GBx8_TRT",
    "Result": 33023.5,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 120,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8580",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "0bf8ef3c6b1a4f65",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5569  ROUGE2: 22.145  ROUGEL: 28.7929  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Dell",
    "Platform": "XE9680_H200_SXM_141GBx8_TRT",
    "Result": 33051.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "422d84eb166b41d2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Dell",
    "Platform": "XE9680_H200_SXM_141GBx8_TRT",
    "Result": 21536.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4ad44cffb3c94c37",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Dell",
    "Platform": "XE9680_H200_SXM_141GBx8_TRT",
    "Result": 21536.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "839180717a904b6c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5036  ROUGE2: 22.0997  ROUGEL: 28.733  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Dell",
    "Platform": "XE9680_H200_SXM_141GBx8_TRT",
    "Result": 19428.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "998ec5f0e0334cd5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5036  ROUGE2: 22.0997  ROUGEL: 28.733  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Dell",
    "Platform": "XE9680_H200_SXM_141GBx8_TRT",
    "Result": 19428.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b89c583f5ed145e5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5569  ROUGE2: 22.145  ROUGEL: 28.7929  TOKENS_PER_SAMPLE: 292.3",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Dell",
    "Platform": "XE9680_H200_SXM_141GBx8_TRT",
    "Result": 33051.9,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8568Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "b28e8579494a4650",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5583  ROUGE2: 22.1382  ROUGEL: 28.7896  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Dell",
    "Platform": "XE7745_H200_NVL_141GBx8_TRT",
    "Result": 28848.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d2abe266ea1a4cd6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Dell",
    "Platform": "XE7745_H200_NVL_141GBx8_TRT",
    "Result": 17975.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "6b9a9e0856b64180",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Dell",
    "Platform": "XE7745_H200_NVL_141GBx8_TRT",
    "Result": 17975.8,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "6a1fa767781c4d28",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5859  ROUGE2: 22.1562  ROUGEL: 28.8145  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Dell",
    "Platform": "XE7745_H200_NVL_141GBx8_TRT",
    "Result": 16134.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "c4d3cf5b2b104157",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5859  ROUGE2: 22.1562  ROUGEL: 28.8145  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Dell",
    "Platform": "XE7745_H200_NVL_141GBx8_TRT",
    "Result": 16134.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "2ce08d78ebf94700",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5583  ROUGE2: 22.1382  ROUGEL: 28.7896  TOKENS_PER_SAMPLE: 292.4",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Dell",
    "Platform": "XE7745_H200_NVL_141GBx8_TRT",
    "Result": 28848.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "88033c5459f54fb1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5724  ROUGE2: 22.1518  ROUGEL: 28.7699  TOKENS_PER_SAMPLE: 292.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 30903.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "129d86eee2044d01",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 21134.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "89d3653fcd944fe5",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.1118  ROUGE2: 20.1071  ROUGEL: 29.9676  GEN_LEN: 4153194",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 21134.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "3674b1a08d054bb0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5968  ROUGE2: 22.1606  ROUGEL: 28.7638  TOKENS_PER_SAMPLE: 293.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99/Server",
    "MlperfModel": "llama2-70b-interactive-99",
    "Model": "llama2-70b-interactive-99",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 12979.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "3df32d67adf244c7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5968  ROUGE2: 22.1606  ROUGEL: 28.7638  TOKENS_PER_SAMPLE: 293.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "MlperfModel": "llama2-70b-interactive-99.9",
    "Model": "llama2-70b-interactive-99.9",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 12979.4,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "fce9c64b750b4888",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-interactive-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5724  ROUGE2: 22.1518  ROUGEL: 28.7699  TOKENS_PER_SAMPLE: 292.9",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Dell",
    "Platform": "XE9680_H100_SXM_80GBx8_TRT",
    "Result": 30903.6,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8468",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "06db1d776ac74b4b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE9680_H100_SXM_80GBx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.0202  ROUGE2: 20.1202  ROUGEL: 29.9957  GEN_LEN: 4052032",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/1-node-2S-GNR_86C/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Dell",
    "Platform": "1-node-2S-GNR_86C",
    "Result": 168.691,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_86C",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "INTEL(R) XEON(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Dell PowerEdge R670. INT4 for GPT-J, and INT8 for all other models",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1",
    "uid": "a96463bd13804ce7",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/1-node-2S-GNR_86C/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "INT8"
  },
  {
    "Accuracy": "ROUGE1: 44.5496  ROUGE2: 22.1516  ROUGEL: 28.7732  TOKENS_PER_SAMPLE: 294.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99/Server",
    "MlperfModel": "llama2-70b-99",
    "Model": "llama2-70b-99",
    "Organization": "Dell",
    "Platform": "XE7745_L40Sx8_TRT",
    "Result": 3201.13,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 384,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "L40S TGP 350W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "4e888c12600b4af0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.055  ROUGE2: 20.1229  ROUGEL: 29.9616  GEN_LEN: 4127336",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99.9/Server",
    "MlperfModel": "gptj-99.9",
    "Model": "gptj-99.9",
    "Organization": "Dell",
    "Platform": "XE7745_L40Sx8_TRT",
    "Result": 6213.34,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 384,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "L40S TGP 350W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "c2a32318820845e0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 43.055  ROUGE2: 20.1229  ROUGEL: 29.9616  GEN_LEN: 4127336",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99/Server",
    "MlperfModel": "gptj-99",
    "Model": "gptj-99",
    "Organization": "Dell",
    "Platform": "XE7745_L40Sx8_TRT",
    "Result": 6213.34,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 384,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "L40S TGP 350W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "239943df56894812",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_L40Sx8_TRT/gptj-99/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  },
  {
    "Accuracy": "ROUGE1: 44.5496  ROUGE2: 22.1516  ROUGEL: 28.7732  TOKENS_PER_SAMPLE: 294.1",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99.9/Server",
    "MlperfModel": "llama2-70b-99.9",
    "Model": "llama2-70b-99.9",
    "Organization": "Dell",
    "Platform": "XE7745_L40Sx8_TRT",
    "Result": 3201.13,
    "Result_Units": "Tokens/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE7745 (8x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Tokens/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 384,
    "host_processor_model_name": "AMD EPYC 9965 192-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "L40S TGP 350W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "bab9ad65b52143c6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/XE7745_L40Sx8_TRT/llama2-70b-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "fp8"
  }
]
