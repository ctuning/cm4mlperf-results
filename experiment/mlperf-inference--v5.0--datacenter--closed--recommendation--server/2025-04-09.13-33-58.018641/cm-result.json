[
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 555631,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "0706e6b1b9a84934",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT",
    "Result": 358055,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with HPE GreenLake for File Storage (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "6d8028e606cc4e68",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H100_SXM_80GBx8_HPE_VAST_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 201011,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "09056bac702947c6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT",
    "Result": 195518,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant DL380a Compute Gen12 (8x H200-NVL-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 192,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "8e25bde285654584",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 80994.3,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "197b18c6d9994f5a",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 50070.5,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.7",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "2d4cf6a476d14f0f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.160",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT",
    "Result": 177643,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "cd499e54287d4fd8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.244",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT",
    "Result": 94989.3,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x L40S-PCIe-48GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "Intel(R) Xeon(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "c208185a80484ca0",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_L40S_PCIe_48GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 590167,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "1cfde8886f8b454f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "HPE_Cray_XD670_H200_SXM_141GBx8_TRT",
    "Result": 380059,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE Cray XD670 with Cray ClusterStor (8x H200-SXM-141GB)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8562Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "RHEL 9.4",
    "uid": "0f4a9dcab37a48f9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT",
    "Result": 170018,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL380a Gen12 (8x H100-NVL-94GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 96,
    "host_processor_model_name": "INTEL(R) XEON(R) 6740E",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "d778fd7cea1d4385",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/HPE_ProLiant_DL380a_H100_NVL_94GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 157961,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "426cc666c40e4270",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "HPE",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT",
    "Result": 98162.1,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1 LTS",
    "uid": "e3c6be2aa748435c",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/HPE/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x2_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Sustainable_Metal_Cloud",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 570145,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "Dell PowerEdge XE9680 (8xH200-SXM-141GBx8, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 32,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8462Y+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "bbf0428de8004b08",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Sustainable_Metal_Cloud/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 555883,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "a617763a394b44a8",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Google",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 362655,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "a3-ultragpu-8g (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "INTEL(R) XEON(R) PLATINUM 8581C CPU",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W. TensorRT LLM",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "58a654ea386d4e3e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Google/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 82792.8,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "f84ca010dd9f4c88",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "S74G_2U_GH200_96GB_aarch64x1_TRT",
    "Result": 45990.4,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid S74G-2U (1x GH200-96GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "QuantaGrid S74G-2U (1x NVIDIA GH200 96GB HBM3)",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.3",
    "uid": "9c5d50e955654653",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/S74G_2U_GH200_96GB_aarch64x1_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 512121,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "e3eecd811c97463b",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D74U_7U_H100_SXM_80GBx8_TRT",
    "Result": 340044,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D74H-7U (8x H100-SXM-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-SXM-80GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "1a573df6179d456d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D74U_7U_H100_SXM_80GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.23785974349185",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "1-node-2S-GNR_86C",
    "Result": 11789.7,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_86C",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "INTEL(R) XEON(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid-D55X-1U. N/A",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.4",
    "uid": "b6b8ce0af0fe411e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 175419,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "16de60d6ca2e4548",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Quanta_Cloud_Technology",
    "Platform": "D54U_3U_H100_PCIe_80GBx4_TRT",
    "Result": 100187,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "QuantaGrid D54U-3U (4x H100-PCIe-80GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-PCIe-80GB",
    "accelerators_per_node": 4,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8.0.43, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 52,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8470",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "QuantaGrid D54U-3U",
    "number_of_nodes": 1,
    "operating_system": "Rocky Linux 9.2",
    "uid": "f50912cf9d124a87",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Quanta_Cloud_Technology/results/D54U_3U_H100_PCIe_80GBx4_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 80994.2,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "85ca15246fce457e",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "NVIDIA",
    "Platform": "GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT",
    "Result": 50991.7,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "HPE ProLiant Compute DL384 Gen12 (1x GH200-144GB_aarch64, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 144GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 144,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 144GB HBM3e",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "689ce5c210664afd",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/GH200-GraceHopper-Superchip_GH200-144GB_aarch64x1_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 585164,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "259e937d8dca41b1",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "NVIDIA",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 370054,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "42f17d4c22ec4065",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/NVIDIA/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "ASUSTeK",
    "Platform": "ESC8000_H200_NVLx8_NVLink_TRT",
    "Result": 525118,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9475F 48-Core Processo",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "80fbf5c89f504b67",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "ASUSTeK",
    "Platform": "ESC8000_H200_NVLx8_NVLink_TRT",
    "Result": 300029,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "ASUSTeK ESC8000A-E13P (8x H200-NVL-141GB NVLink, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-NVL-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 48,
    "host_processor_model_name": "AMD EPYC 9475F 48-Core Processo",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d1aead002ed34242",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/ASUSTeK/results/ESC8000_H200_NVLx8_NVLink_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 585166,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "6665a91880f646e9",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "GigaComputing",
    "Platform": "G893-SD1_H200-SXM-141GBx8_TRT",
    "Result": 370056,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "G893-SD1",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480+",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04.4",
    "uid": "9fe65bcf6cd347d6",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/GigaComputing/results/G893-SD1_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.23785974349185",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Intel",
    "Platform": "1-node-2S-GNR_128C",
    "Result": 18177,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_128C",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "INTEL(R) XEON(R) 6980P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "N/A. N/A",
    "number_of_nodes": 1,
    "operating_system": "CentOS Stream 9",
    "uid": "4b51dac4a7994276",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.23785974349185",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Intel",
    "Platform": "1-node-2S-GNR_128C",
    "Result": 18177,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_128C",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "INTEL(R) XEON(R) 6980P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "N/A. N/A",
    "number_of_nodes": 1,
    "operating_system": "CentOS Stream 9",
    "uid": "5a700c5fa9d04139",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.164",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/X215M8-L40Sx2_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Cisco",
    "Platform": "X215M8-L40Sx2_TRT",
    "Result": 49591.6,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "UCS X215 M8 (2x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9224 24-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "a5b8306eb2fa4221",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/X215M8-L40Sx2_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.162",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/L40Sx2_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Cisco",
    "Platform": "L40Sx2_TRT",
    "Result": 49591.6,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "UCS C245 M8 (2x L40S, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA L40S",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 64,
    "host_processor_model_name": "AMD EPYC 9684X 128-Core Processor",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "022a708cb1d543aa",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/L40Sx2_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Cisco",
    "Platform": "C245M8_H100NVL_94GBx2_TRT",
    "Result": 96987.8,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS C245 M8 (2x H100NVL-PCIe-94GB)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 24,
    "host_processor_model_name": "AMD EPYC 9224",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "d846c7b6a7364c02",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/C245M8_H100NVL_94GBx2_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Cisco/results/X215M8_H100NVLx2_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Cisco",
    "Platform": "X215M8_H100NVLx2_TRT",
    "Result": 98585.8,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "Cisco UCS X215c M8 with X440P PCIe node (2x H100NVL-PCIe-94GB)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H100-NVL-94GB",
    "accelerators_per_node": 2,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "AMD EPYC 9554",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "697fc46416f6491f",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Cisco/results/X215M8_H100NVLx2_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 585164,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "50238e3fd91849b2",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Lambda",
    "Platform": "H200-SXM-141GBx8_TRT",
    "Result": 370055,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "NVIDIA H200 (8x H200-SXM-141GB, TensorRT)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA H200-SXM-141GB",
    "accelerators_per_node": 8,
    "compliance": 0,
    "errors": 0,
    "framework": "TensorRT 10.8, CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 56,
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8480C",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "H200 TGP 700W",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "719023117c48459d",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Lambda/results/H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.166",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99/Server",
    "MlperfModel": "dlrm-v2-99",
    "Model": "dlrm-v2-99",
    "Organization": "Supermicro",
    "Platform": "ARS_111GL_NHR_TRT",
    "Result": 80992.9,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "ARS-111GL-NHR (1x GH200-96GB_aarch64)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 96GB HBM3",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "cac1563f4dd14711",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.233",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Supermicro",
    "Platform": "ARS_111GL_NHR_TRT",
    "Result": 50468.9,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "ARS-111GL-NHR (1x GH200-96GB_aarch64)",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "NVIDIA GH200 Grace Hopper Superchip 96GB",
    "accelerators_per_node": 1,
    "compliance": 0,
    "errors": 0,
    "framework": "CUDA 12.8",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 72,
    "host_processor_model_name": "NVIDIA Grace CPU",
    "host_processors_per_node": 1,
    "inferred": 0,
    "notes": "NVIDIA GH200 96GB HBM3",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04",
    "uid": "1867b32aecaf49fa",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/ARS_111GL_NHR_TRT/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.23785974349185",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Supermicro/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Supermicro",
    "Platform": "1-node-2S-GNR_128C",
    "Result": 18987,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "SYS-822GA-NGR3",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 128,
    "host_processor_model_name": "INTEL(R) XEON(R) 6980P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "N/A",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 22.04",
    "uid": "00ae783f01584b00",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Supermicro/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  },
  {
    "Accuracy": "AUC: 80.23785974349185",
    "Availability": "available",
    "Division": "closed",
    "Location": "closed/Dell/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server",
    "MlperfModel": "dlrm-v2-99.9",
    "Model": "dlrm-v2-99.9",
    "Organization": "Dell",
    "Platform": "1-node-2S-GNR_86C",
    "Result": 11788.9,
    "Result_Units": "Queries/s",
    "Scenario": "Server",
    "SystemName": "1-node-2S-GNR_86C",
    "SystemType": "datacenter",
    "Units": "Queries/s",
    "accelerator_model_name": "N/A",
    "accelerators_per_node": 0,
    "compliance": 0,
    "errors": 0,
    "framework": "PyTorch",
    "git_url": "https://github.com/mlcommons/inference_results_v5.0",
    "has_power": false,
    "host_processor_core_count": 86,
    "host_processor_model_name": "INTEL(R) XEON(R) 6787P",
    "host_processors_per_node": 2,
    "inferred": 0,
    "notes": "Dell PowerEdge R670. INT4 for GPT-J, and INT8 for all other models",
    "number_of_nodes": 1,
    "operating_system": "Ubuntu 24.04.1",
    "uid": "016ec2958cef4da4",
    "url": "https://github.com/mlcommons/inference_results_v5.0/tree/master/closed/Dell/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server",
    "version": "v5.0",
    "weight_data_types": "int8"
  }
]
